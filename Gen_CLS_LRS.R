args = commandArgs(TRUE)
ML_Type = args[1]   # Reg or Class
Gene_Type = args[2]   # Common or Rare 
Feature_Size<-args[3]#as.numeric(eval(parse(text=args[3])))
P.causal<-as.numeric(eval(parse(text=args[4])))
h2<-as.numeric(eval(parse(text=args[5])))
CC<-as.numeric(eval(parse(text=args[6])))
#n.G <-Feature_Size # number of features
n.G <-2000

m<-10000 # sample size
effect <- 0;#0.4; 
#P.causal <- 5;
#h2<- 0.2
corr_max<- 0.75
#CC <- 25
print(paste0("Current h2 - herediability is: ", h2))

indx <- Sys.getenv("SLURM_ARRAY_TASK_ID")
print(indx)

source('/oak/stanford/groups/zihuai/Peyman/DeepPinks/Mul_DP/Top_Med/KnockoffScreen.R')

#Load SKAT haplotypes
library(SKAT)
library(Matrix)
library(knockoff)
library(WGScan)
library(numDeriv)
library(glmnet)
library(SPAtest)
library(CompQuadForm)
library(data.table)
library(irlba) 
library(bigmemory)
library(dplyr, warn.conflicts = FALSE)
library(tidyr, warn.conflicts = FALSE)


`+` <- function(e1, e2) {
  if (is.character(e1) | is.character(e2)) {
    paste0(e1, e2)
  } else {
    base::`+`(e1, e2)
  }
}

implode <- function(..., sep='') {
  paste(..., collapse=sep)
}

prt1 = '/oak/stanford/groups/zihuai/Peyman/DeepPinks/Mul_DP'
prt_dest = prt1

vec <- c(prt1, ML_Type, Gene_Type, Feature_Size)
print(vec)
prt1 = implode(vec, sep='/')
prt1 = prt1 + '/'
print(prt1)


prt2 = 'X_orig_' + indx + '.csv'
prt3 = 'Y_'+ indx +'.csv'
prt4 = 'Beta_'+ indx +'.csv'
prt51 = 'X_ko1_' + indx + '.csv'
prt52 = 'X_ko2_' + indx + '.csv'
prt53 = 'X_ko3_' + indx + '.csv'
prt54 = 'X_ko4_' + indx + '.csv'
prt55 = 'X_ko5_' + indx + '.csv'
Path_orig = prt1 + prt2
Path_Y = prt1 + prt3
Path_B = prt1 + prt4
Path_ko1 = prt1 + prt51
Path_ko2 = prt1 + prt52
Path_ko3 = prt1 + prt53
Path_ko4 = prt1 + prt54
Path_ko5 = prt1 + prt55


data(SKAT.haplotypes)
data<-SKAT.haplotypes$Haplotype;
info<-SKAT.haplotypes$SNPInfo;
# a numeric matrix of 10,000 haplotypes over 200k BP region. Each row represents a
# different haplotype, and each column represents a different SNP marker. It is generated by the
# calibration coalescent model (COSI) with mimicking LD structure of European ancestry.
#hist(info[,5],main="Histogram of MAF over 200kb region",xlab='MAF')


#Generate SNP sets
SNP.index<-sort(sample(1:ncol(data),n.G))
sample.index<-sample(1:nrow(data),m*2,replace=T)
SNP.set<-data[sample.index[1:m],SNP.index]+data[sample.index[(m+1):(2*m)],SNP.index]#generate genotype from haplotype
pos<-info[SNP.index,3]
MAF<-apply(SNP.set,2,mean)/2

SNP.set[,MAF>0.5]<-2-SNP.set[,MAF>0.5]
MAF<-apply(SNP.set,2,mean)/2
MAC<-apply(SNP.set,2,sum)

#if(Gene_Type=='Common'){rare.index<-MAF>0.01 & MAF>0}else{
#  rare.index<-MAF<0.01 & MAF>0
#}

#########################################################
if (Gene_Type == 'Common'){
rare.index<-MAF>0.01 & MAF>0   # Common
} else if (Gene_Type == 'Rare'){
rare.index<-(MAF<0.01 & MAF>0 & MAC>10)
} else {
rare.index<-(MAF>0 & MAC>10)
}
 # RARE    #MAC>10#MAF>0.001 & MAF>0 #Common 
#########################################################

#rare.index<-(MAF>0.01 & MAF>0)      #MAC>10#MAF>0.001 & MAF>0 #Common 
SNP.set<-SNP.set[,rare.index];
MAF<-MAF[rare.index];
MAC<-MAC[rare.index];
pos<-pos[rare.index]
N.SNP<-ncol(SNP.set)

cor.X<-sparse.cor(Matrix(SNP.set))$cor
Sigma.distance = as.dist(1 - abs(cor.X))
fit = hclust(Sigma.distance, method="single")

clusters = cutree(fit, h=1-corr_max)
cluster.index<-match(unique(clusters),clusters)
SNP.set<-SNP.set[,cluster.index];
MAF<-MAF[cluster.index];
MAC<-MAC[cluster.index];
pos<-pos[cluster.index]
N.SNP<-ncol(SNP.set)

#m<-nrow(SNP.set);n.G<-500
G<-SNP.set#[,sample(1:ncol(SNP.set),500)]
#pos<-as.numeric(sub(".*[.]", "", colnames(G)))

nrow(G)
ncol(G)

##############################################
# Remove columns with zero std
G <- G[, which(colSums(G) != 0)]
G<-G[, which(colSums(G) != nrow(G))]
G<-G[, which(colSums(G) != 2*nrow(G))]
##############################################

nrow(G)
ncol(G)

print(sum(is.na(G)))

MAF<-apply(G,2,mean)/2
MAC<-apply(G,2,sum)
N.SNP<-ncol(G)

cov.G<-cor(G)
#hist(apply(abs(cov.G)>0.5,1,sum)-1)

print("The mean of correlated variables is:")
mean((apply(abs(cov.G)>0.5,1,sum)-1)>0) # [1] 0.21. 21% variables are highly correlated to at least one other variable

result.all<-c();

#generate causal
start<-min(pos,1);
end<-max(pos,1);
#start+signal.region

signal.index<-which(pos>=start & pos<=end)
causal.index<-(1:N.SNP)%in%sample(signal.index,P.causal) #sample(signal.index,ceiling(length(signal.index)*P.causal))

#################### G.Interactions ############################################

#NZ = which(causal.index, arr.ind = FALSE, useNames = TRUE)
#G_new = G;
#for (i in 1:(P.causal-1)){
#  for (j in (i+1):P.causal){
#    G_int = G[,NZ[[i]]]*G[,NZ[[j]]] 
#    G_new = cbind(G_new,G_int)
#  }
#}

####################################################################################################
#causal.index<-(1:N.SNP)%in%sample(1:ncol(G),k)
a=sqrt(h2/(1-h2)/sum((as.matrix(abs(log10(MAF)))*causal.index)^2*apply(G,2,var)));if(a==Inf){a<-0}
Beta<-as.matrix(a*abs(log10(MAF)))*causal.index#[causal.index==1])
################# Interaction Coefficients #########################################################

Temp = which( causal.index!= FALSE, arr.ind=TRUE)

Beta[Temp[1]] = -1*Beta[Temp[1]];
#Beta[Temp[4]] = -1*Beta[Temp[4]];


#beta_new = Beta;
#for (i in 1:(P.causal-1)){
#  for (j in (i+1):P.causal){
#    beta_int = as.matrix(effect*a*abs(log10(MAF[NZ[i]]))*abs(log10(MAF[NZ[j]])));  
#    beta_new = rbind(beta_new,beta_int)
#  }
#}
#print(beta_new)
#############################################################

#Generate phenotype
Prevalence<-0.1;#;OR.f1<-9;OR.f2<-1#2
b0_pheno<-log(Prevalence/(1-Prevalence))#for binary trait
X1<-rnorm(m,0,1)
mu <- b0_pheno + CC*((X1+G%*%Beta)^2)
probab = exp(mu+b0_pheno)/(1+exp(mu+b0_pheno));

y<-as.matrix((rbinom(m,1,probab)))

ind_na = which(is.na(y))

if(!length(ind_na)==0) {
	y<-y[-c(ind_na),]
	G<-G[-c(ind_na),]
} 

y_new <- y

#mu_new<-X1+G_new%*%beta_new

#y_new<-as.matrix(rbinom(m,1, exp(mu_new+b0_pheno)/(1+exp(mu_new+b0_pheno))))

#print("The number of target values changed after adding interaction terms is:")
#setdiff(y,y_new)

#G_k<-create.MK(G,pos,M=5,corr_max)
#TEMP_GK = create.MK.AL(G,pos,M=5,corr_max);
TEMP_GK<-create.MK.AL(G,pos,M=5,maxN.neighbor=Inf,maxBP.neighbor=Inf,corr_base=0.05,n.AL=nrow(G),thres.ultrarare=10,R2.thres=1,method='shrinkage',bigmemory=F)

G_k1 <- TEMP_GK[[1]]
G_k2 <- TEMP_GK[[2]]
G_k3 <- TEMP_GK[[3]]
G_k4 <- TEMP_GK[[4]]
G_k5 <- TEMP_GK[[5]]
	
nrow(G_k1)
ncol(G_k1)

write.table(G_k1,Path_ko1, sep=',', col.names=F,   row.names=F)
write.table(G_k2,Path_ko2, sep=',', col.names=F,   row.names=F)
write.table(G_k3,Path_ko3, sep=',', col.names=F,   row.names=F)
write.table(G_k4,Path_ko4, sep=',', col.names=F,   row.names=F)
write.table(G_k5,Path_ko5, sep=',', col.names=F,   row.names=F)
write.table(y_new,Path_Y, sep=',', col.names=F,   row.names=F)
write.table(Beta,Path_B, sep=',', col.names=F,   row.names=F)
write.table(G,Path_orig, sep=',', col.names=F,   row.names=F)

print("End of data generation")


########################################################

#######################  LASSO  ########################

########################################################
library(AUC)
library(pROC)

Temp = which( Beta!= 0, arr.ind=TRUE)
NZ_True = Temp
print(NZ_True)
p = NROW(Beta)
X_ko = G_k1;
X = G;
y <- y_new;

#smp_size <- 0.9 * nrow(X)
## set the seed to make your partition reproducible
#set.seed(123)
#train_ind <- sample(seq_len(nrow(X)), size = smp_size)

#####################################################################################
##############         Apply Lasso on Joint data X and X_ko        ##################
#####################################################################################

X_all = cbind(X,X_ko);
X_all <- scale(X_all, scale = TRUE);

#x_trainJ <- X_all[train_ind, ]
#x_testJ <- X_all[-train_ind, ]
#y_train <- y[train_ind]
#y_test <- y[-train_ind]

#lasso_modJ = glmnet(x_trainJ, y_train, family = "binomial", type.measure = "auc", alpha = 1) 
#lasso_modJ = glmnet(X_all, y, family = "binomial", type.measure = "auc", alpha = 1) 


#plot(lasso_modJ)    # Draw plot of coefficients
cv.outJ = cv.glmnet(X_all, y, family = "binomial", type.measure = "auc", alpha = 1, nfolds = 5) 
#plot(cv.outJ) # Draw plot of training MSE as a function of lambda
bestlamJ = cv.outJ$lambda.min 

ind_best <- which(cv.outJ$lambda == bestlamJ)
Max_AUC_Val <- cv.outJ$cvm[ind_best]

paste("Best Lambda Lasso classification is:", bestlamJ) 
paste("AUC validation Lasso Classification is", Max_AUC_Val) 

#lasso_predJ = predict(lasso_modJ, s = bestlamJ, newx = x_trainJ) # Use best lambda to predict train data
#roc_obj_tr <- roc(y_train, as.numeric(lasso_predJ))
#AUC_tr = auc(roc_obj_tr)
#paste("AUC train Lasso Classification is", AUC_tr) 
#y_test <- factor(y_test)
#auc(roc(lasso_predJ,y_test))
#lasso.mod2=glmnet(X,y,alpha=1, lambda=seq(10, 1000, 1))
#plot(lasso_modJ) 

#Lasso_pred_Te = predict(lasso_modJ, s = bestlamJ, newx = x_testJ) # Use best lambda to predict test data
#roc_obj_te <- roc(y_test, as.numeric(Lasso_pred_Te))
#AUC_test = auc(roc_obj_te)
#paste("AUC test Lasso Classification is", AUC_test) 

######################
#new_lineJ = t(c(AUC_tr,AUC_test, indx))
new_lineJ = t(c(Max_AUC_Val, indx))

DestinationS <- prt_dest + '/'+ ML_Type + '/'+ Gene_Type + '/' + 'Metrics_Lasso_' + Feature_Size + '_H2_'+ h2 + '_C_' + CC + '_PCausal_'+ P.causal + '.csv'
if (file.exists(DestinationS) == FALSE) 
  file.create(file=DestinationS)

write.table(new_lineJ,file=DestinationS, append=TRUE, sep=',', eol="\n", col.names=F, row.names=F)

######################

outJ = glmnet(X_all, y, family = "binomial", type.measure = "auc", alpha = 1) 
lasso_coef_J <- predict(outJ, type = "coefficients", s = bestlamJ)[1:(2*p+1),] 
W_J = abs(lasso_coef_J[2:(p+1)]) - abs(lasso_coef_J[(p+2):(2*p+1)])

# Threshold 
Target_FDR <-seq(0.01, 0.2, by=0.01)  #0.005
count <- 0
FDR_L <- numeric(length = length(Target_FDR))
Power_L <- numeric(length = length(Target_FDR))

for (val in Target_FDR) {
  count = count+1
  T_J = knockoff.threshold(W_J, fdr=val)
  Inds_J = W_J > T_J
  
  Temp_J = which( Inds_J!= FALSE, arr.ind=TRUE)
  NZ_Sel_J = Temp_J
  Total_sel_J = NROW(NZ_Sel_J)
  
  FDR_L[count] = sum(Beta[NZ_Sel_J] == 0)/max(1,Total_sel_J)
  Power_L[count] = sum(Beta[NZ_Sel_J] != 0)/sum(Beta != 0)
}
#if(val %% 2 == 0)  count = count+1
#print(count)
#plot(Target_FDR, FDR_L, pch=16, ylim=c(0,1), type="b") # first plot
print("Joint Feature Lasso")
print(FDR_L)
print(Power_L)

new_lineJ = t(c(FDR_L,Power_L, Max_AUC_Val, indx))

DestinationL <- prt_dest + '/'+ ML_Type + '/'+ Gene_Type + '/' + 'Stat_LassoJ_' + Feature_Size + '_H2_'+ h2 + '_C_' + CC + '_PCausal_'+ P.causal + '.csv'
if (file.exists(DestinationL) == FALSE) 
  file.create(file=DestinationL)
write.table(new_lineJ,file=DestinationL, append=TRUE, sep=',', eol="\n", col.names=F, row.names=F)



########################################################

##############         Ridge        ##################

########################################################

Ridge_modk = glmnet(X_all, y, family = "binomial", type.measure = "auc", alpha = 0) 


#plot(Ridge_modk)    # Draw plot of coefficients
cv.outk = cv.glmnet(X_all, y, family="binomial", type.measure = "auc",alpha = 0, nfolds = 5) 

#plot(cv.out) # Draw plot of training MSE as a function of lambda
bestlamk = cv.outk$lambda.min # Select lamda that minimizes training MSE

ind_bestk <- which(cv.outk$lambda == bestlamk)
Max_AUC_Val <- cv.outk$cvm[ind_bestk]

paste("Best Lambda ridge classification is:", bestlamk) 
paste("AUC validation Lasso Classification is", Max_AUC_Val) 


Ridge_pred_Tr = predict(Ridge_modk, s = bestlamk, newx = X_all) 
roc_obj_tr <- roc(y, as.numeric(Ridge_pred_Tr))
AUC_tr = auc(roc_obj_tr)
paste("AUC train Ridge Classification is", AUC_tr) 


Ridge_pred_Te = predict(Ridge_modk, s = bestlamk, newx = X_all) # Use best lambda to predict test data
roc_obj_te <- roc(y, as.numeric(Ridge_pred_Te))
AUC_test = auc(roc_obj_te)
paste("AUC test Ridge Classification is", AUC_test) 

######################
new_lineRidge = t(c(Max_AUC_Val, indx))

DestinationR <- prt_dest + '/'+ ML_Type + '/'+ Gene_Type + '/' + 'Metrics_Ridge_' + Feature_Size + '_H2_'+ h2 + '_C_' + CC + '_PCausal_'+ P.causal + '.csv'
if (file.exists(DestinationR) == FALSE) 
  file.create(file=DestinationR)

write.table(new_lineRidge,file=DestinationR, append=TRUE, sep=',', eol="\n", col.names=F, row.names=F)

######################

#lasso.mod2=glmnet(X,y,alpha=0, lambda=seq(10, 1000, 1))
#plot(Ridge_modJ) 
outJ = glmnet(X_all, y, family="binomial", type.measure = "auc",alpha = 0) 

Ridge_coef_J <- predict(outJ, type = "coefficients", s = bestlamk)[1:(2*p+1),] # Display coefficients using lambda chosen by CV

W_J = abs(Ridge_coef_J[2:(p+1)]) - abs(Ridge_coef_J[(p+2):(2*p+1)])
# Threshold 
Target_FDR <-seq(0.01, 0.2, by=0.01) 
count <- 0
FDR_R <- numeric(length = length(Target_FDR))
Power_R <- numeric(length = length(Target_FDR))

for (val in Target_FDR) {
  count = count+1
  T_J = knockoff.threshold(W_J, fdr=val)
  Inds_J = W_J > T_J
  
  Temp_J = which( Inds_J!= FALSE, arr.ind=TRUE)
  NZ_Sel_J = Temp_J
  Total_sel_J = NROW(NZ_Sel_J)
  
  FDR_R[count] = sum(Beta[NZ_Sel_J] == 0)/max(1,Total_sel_J)
  Power_R[count] = sum(Beta[NZ_Sel_J] != 0)/sum(Beta != 0)
}
#if(val %% 2 == 0)  count = count+1
#print(count)
#plot(Target_FDR, FDR_R, pch=16, ylim=c(0,1), type="b") # first plot
print("Joint Feature Ridge")
print(FDR_R)
print(Power_R)

new_lineJ = t(c(FDR_R,Power_R, Max_AUC_Val, indx))

DestinationR <- prt_dest + '/'+ ML_Type + '/'+ Gene_Type + '/' + 'Stat_Ridge_' + Feature_Size + '_H2_'+ h2 + '_C_' + CC + '_PCausal_'+ P.causal + '.csv'
if (file.exists(DestinationR) == FALSE) 
  file.create(file=DestinationR)

write.table(new_lineJ,file=DestinationR, append=TRUE, sep=',', eol="\n", col.names=F, row.names=F)

########################################################

##############         SVM        ##################

########################################################

library(LiblineaR)

#X_all = cbind(X,X_ko)
#x_trainJ <- X_all[train_ind, ]
#x_testJ <- X_all[-train_ind, ]
#y_train <- y[train_ind]
#y_test <- y[-train_ind]

tryCosts=c(10000, 1000, 500, 200, 100, 50, 40, 30, 20, 10, 5, 1, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00001)
bestCost=NA
bestAcc=0

for(co in tryCosts){
	    acc_tr = LiblineaR(data = X_all, target = y, cost=co, type = 2, bias = 1, cross=5) # epsilon=0.01
        cat("Results for C=",co," : ",acc_tr," accuracy.\n",sep="")
	    if(acc_tr>bestAcc){
			      bestCost=co
			      bestAcc=acc_tr
		}
}		

cat("Best cost is:",bestCost,"\n")
cat("Best accuracy is:",bestAcc,"\n") 

# Build the model
model_svm = LiblineaR(data = X_all, target = y,cost=bestCost, bias = 1, type = 2, cross=0) # epsilon=0.01

pr = FALSE
pred_te = predict(model_svm,X_all,proba=pr,decisionValues=TRUE)

# Display confusion matrix
res=table(pred_te$predictions,y)
print(res)


######################
new_lineSVM = t(c(bestAcc,bestCost, indx))

DestinationS <- prt_dest + '/'+ ML_Type + '/'+ Gene_Type + '/' + 'Metrics_SVM_' + Feature_Size + '_H2_'+ h2 + '_C_' + CC + '_PCausal_'+ P.causal + '.csv'
if (file.exists(DestinationS) == FALSE) 
  file.create(file=DestinationS)

write.table(new_lineSVM,file=DestinationS, append=TRUE, sep=',', eol="\n", col.names=F, row.names=F)

######################


#pred=predict(model_svm,x_testJ)$predictions
#MSETest=mean((y_test-pred)^2)

# Distribution of errors
#print(summary(y_test-pred))

coef_J <- model_svm$W
W_J = abs(coef_J[1:(p)]) - abs(coef_J[(p+1):(2*p)])

#T_J = knockoff.threshold(W_J, fdr=0.1)
#Inds_J = W_J > T_J
#Temp_J = which( Inds_J!= FALSE, arr.ind=TRUE)
#NZ_Sel_J = Temp_J
#Total_sel_J = NROW(NZ_Sel_J)
#FDR_S = sum(Beta[NZ_Sel_J] == 0)/max(1,Total_sel_J)
#Power_S = sum(Beta[NZ_Sel_J] != 0)/sum(Beta != 0)

Target_FDR <-seq(0.01, 0.2, by=0.01) 
count <- 0
FDR_S <- numeric(length = length(Target_FDR))
Power_S <- numeric(length = length(Target_FDR))

for (val in Target_FDR) {
  count = count+1
  T_J = knockoff.threshold(W_J, fdr=val)
  Inds_J = W_J > T_J
  
  Temp_J = which( Inds_J!= FALSE, arr.ind=TRUE)
  NZ_Sel_J = Temp_J
  Total_sel_J = NROW(NZ_Sel_J)
  
  FDR_S[count] = sum(Beta[NZ_Sel_J] == 0)/max(1,Total_sel_J)
  Power_S[count] = sum(Beta[NZ_Sel_J] != 0)/sum(Beta != 0)
}
#if(val %% 2 == 0)  count = count+1
#print(count)
#plot(Target_FDR, FDR_S, pch=16, ylim=c(0,1), type="b") # first plot
print("Joint Feature")
print(FDR_S)
print(Power_S)
new_lineSS = t(c(FDR_S,Power_S, indx))

DestinationSS <- prt_dest + '/'+ ML_Type + '/'+ Gene_Type + '/' + 'Stat_SVM_' + Feature_Size + '_H2_'+ h2 + '_C_' + CC + '_PCausal_'+ P.causal + '.csv'
if (file.exists(DestinationSS) == FALSE) 
  file.create(file=DestinationSS)

write.table(new_lineSS,file=DestinationSS, append=TRUE, sep=',', eol="\n", col.names=F, row.names=F)
