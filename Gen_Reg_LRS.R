args = commandArgs(TRUE)
ML_Type = args[1]   # Reg or Class
Gene_Type = args[2]   # Common or Rare 
Feature_Size<-args[3]#as.numeric(eval(parse(text=args[3])))
P.causal<-as.numeric(eval(parse(text=args[4])))
h2<-as.numeric(eval(parse(text=args[5])))
CC<-as.numeric(eval(parse(text=args[6])))
#n.G <-Feature_Size # number of features
n.G <-2000

m<-10000 # sample size
effect <- 0;#0.4; 
#P.causal <- 5;
#h2<- 0.2
corr_max<- 0.75
#CC <- 25
print(paste0("Current h2 - herediability is: ", h2))
print(paste0("Current CC - constant value is: ", CC))
print(paste0("Number of Causal signals - is: ", P.causal))

indx <- Sys.getenv("SLURM_ARRAY_TASK_ID")
print(indx)

source('/oak/stanford/groups/zihuai/Peyman/DeepPinks/Mul_DP/Top_Med/KnockoffScreen_AL_vMar16-updated.R')
#source('/oak/stanford/groups/zihuai/Peyman/DeepPinks/Mul_DP/Top_Med/KnockoffScreen_updated.R')


#Load SKAT haplotypes
library(SKAT)
library(Matrix)
library(knockoff)
library(WGScan)
library(numDeriv)
library(glmnet)
library(SPAtest)
library(CompQuadForm)
library(data.table)
library(dplyr, warn.conflicts = FALSE)
library(tidyr, warn.conflicts = FALSE)
library(irlba) 
library(bigmemory)

`+` <- function(e1, e2) {
  if (is.character(e1) | is.character(e2)) {
    paste0(e1, e2)
  } else {
    base::`+`(e1, e2)
  }
}

implode <- function(..., sep='') {
  paste(..., collapse=sep)
}

prt1 = '/oak/stanford/groups/zihuai/Peyman/DeepPinks/Mul_DP/'
prt_dest = prt1

vec <- c(prt1, ML_Type, Gene_Type, Feature_Size)
print(vec)
prt1 = implode(vec, sep='/')
prt1 = prt1 + '/'
print(prt1)


prt2 = 'X_orig_' + indx + '.csv'
prt3 = 'Y_'+ indx +'.csv'
prt4 = 'Beta_'+ indx +'.csv'
prt51 = 'X_ko1_' + indx + '.csv'
prt52 = 'X_ko2_' + indx + '.csv'
prt53 = 'X_ko3_' + indx + '.csv'
prt54 = 'X_ko4_' + indx + '.csv'
prt55 = 'X_ko5_' + indx + '.csv'
Path_orig = prt1 + prt2
Path_Y = prt1 + prt3
Path_B = prt1 + prt4
Path_ko1 = prt1 + prt51
Path_ko2 = prt1 + prt52
Path_ko3 = prt1 + prt53
Path_ko4 = prt1 + prt54
Path_ko5 = prt1 + prt55


data(SKAT.haplotypes)
data<-SKAT.haplotypes$Haplotype;  info<-SKAT.haplotypes$SNPInfo
# a numeric matrix of 10,000 haplotypes over 200k BP region. Each row represents a
# different haplotype, and each column represents a different SNP marker. It is generated by the
# calibration coalescent model (COSI) with mimicking LD structure of European ancestry.
#hist(info[,5],main="Histogram of MAF over 200kb region",xlab='MAF')

#Generate SNP sets
SNP.index<-sort(sample(1:ncol(data),n.G))
sample.index<-sample(1:nrow(data),m*2,replace=T)
SNP.set<-data[sample.index[1:m],SNP.index]+data[sample.index[(m+1):(2*m)],SNP.index]#generate genotype from haplotype
pos<-info[SNP.index,3]
MAF<-apply(SNP.set,2,mean)/2

SNP.set[,MAF>0.5]<- 2-SNP.set[,MAF>0.5]
MAF<-apply(SNP.set,2,mean)/2
MAC<-apply(SNP.set,2,sum)

#if(Gene_Type=='Common'){rare.index<-MAF>0.01 & MAF>0}else{
#  rare.index<-MAF<0.01 & MAF>0
#}

#####################################################################################
if (Gene_Type == 'Common'){
rare.index<-MAF>0.01 & MAF>0   # Common
} else if (Gene_Type == 'Rare'){
rare.index<-(MAF<0.01 & MAF>0 & MAC>10)
} else {
rare.index<-(MAF>0 & MAC>10)
}
 # RARE    #MAC>10#MAF>0.001 & MAF>0 #Common 
######################################################################################

SNP.set<-SNP.set[,rare.index];MAF<-MAF[rare.index];MAC<-MAC[rare.index];pos<-pos[rare.index]
N.SNP<-ncol(SNP.set)
cor.X<-sparse.cor(Matrix(SNP.set))$cor
Sigma.distance = as.dist(1 - abs(cor.X))
fit = hclust(Sigma.distance, method="single")
clusters = cutree(fit, h=1-corr_max)
cluster.index<-match(unique(clusters),clusters)
SNP.set<-SNP.set[,cluster.index];
MAF<-MAF[cluster.index];
MAC<-MAC[cluster.index];
pos<-pos[cluster.index]
N.SNP<-ncol(SNP.set)

#m<-nrow(SNP.set);n.G<-500
G<-SNP.set#[,sample(1:ncol(SNP.set),500)]
#pos<-as.numeric(sub(".*[.]", "", colnames(G)))


nrow(G)
ncol(G)

##############################################
# Remove columns with zero std
G <- G[, which(colSums(G) != 0)]
G<-G[, which(colSums(G) != nrow(G))]
G<-G[, which(colSums(G) != 2*nrow(G))]
##############################################

nrow(G)
ncol(G)

print(sum(is.na(G)))

MAF<-apply(G,2,mean)/2
MAC<-apply(G,2,sum)
N.SNP<-ncol(G)

cov.G<-cor(G)
#hist(apply(abs(cov.G)>0.5,1,sum)-1)

print("The mean of correlated variables is:")
mean((apply(abs(cov.G)>0.5,1,sum)-1)>0) # [1] 0.21. 21% variables are highly correlated to at least one other variable

result.all<-c();

#generate causal
start<-min(pos,1);
end<-max(pos,1);
#start+signal.region

signal.index<-which(pos>=start & pos<=end)
causal.index<-(1:N.SNP)%in%sample(signal.index,P.causal) #sample(signal.index,ceiling(length(signal.index)*P.causal))

#################### G.Interactions ############################################

#NZ = which(causal.index, arr.ind = FALSE, useNames = TRUE)
#G_new = G;
#for (i in 1:(P.causal-1)){
#  for (j in (i+1):P.causal){
#    G_int = G[,NZ[[i]]]*G[,NZ[[j]]] 
#    G_new = cbind(G_new,G_int)
#  }
#}

####################################################################################################
#causal.index<-(1:N.SNP)%in%sample(1:ncol(G),k)
a=sqrt(h2/(1-h2)/sum((as.matrix(abs(log10(MAF)))*causal.index)^2*apply(G,2,var)));if(a==Inf){a<-0}
Beta<-as.matrix(a*abs(log10(MAF)))*causal.index  #[causal.index==1])
################# Interaction Coefficients #########################################################

Temp = which( causal.index!= FALSE, arr.ind=TRUE)

Beta[Temp[1]] = -1*Beta[Temp[1]];
#Beta[Temp[2]] = -1*Beta[Temp[2]];

#Beta[Temp[1]] = -1*Beta[Temp[1]];
#Beta[Temp[3]] = -1*Beta[Temp[3]];
#Beta[Temp[4]] = -1*Beta[Temp[5]];

#beta_new = Beta;
#for (i in 1:(P.causal-1)){
#  for (j in (i+1):P.causal){
#    beta_int = as.matrix(effect*a*abs(log10(MAF[NZ[i]]))*abs(log10(MAF[NZ[j]])));  
#    beta_new = rbind(beta_new,beta_int)
#  }
#}
#print (beta_new)
#############################################################

#############*****************************************************######################
#Generate phenotype
X1<-rnorm(m,0,1)
mu <- CC*(G%*%Beta)^2
y<-as.matrix(mu+X1+rnorm(m,0,2))

#mu <- CC*(G%*%Beta)^2#
#mu <- CC*(exp(G%*%Beta))
#mu<-X1+rnorm(m,0,2)
#mu <- CC*((X1+rnorm(m,0,2))^2)
#print(mu)
#y<-as.matrix(CC*(exp(mu+G%*%Beta)))
#y<-as.matrix(mu+G%*%Beta)
#y<-as.matrix(mu+X1+rnorm(m,0,2))
#############*****************************************************######################

y_new <- y;

#y_new<-as.matrix((mu+(G_new%*%beta_new))^3)
#print("The number of target values changed after adding interaction terms is:")
#setdiff(y,y_new)

#Z<-as.matrix(rep(1,ncol(G)));


#G_k<-create.MK(G,pos,M=5,corr_max)
#G_k1 <- G_k[1,,]; 
#G_k2 <- G_k[2,,];
#G_k3 <- G_k[3,,];
#G_k4 <- G_k[4,,];
#G_k5 <- G_k[5,,];

#TEMP_GK = create.MK.AL(G,pos,M=5,corr_max);
TEMP_GK<-create.MK.AL(G,pos,M=5,maxN.neighbor=Inf,maxBP.neighbor=Inf,corr_base=0.05,n.AL=nrow(G),thres.ultrarare=10,R2.thres=1,method='shrinkage',bigmemory=F)

G_k1 <- TEMP_GK[[1]]
G_k2 <- TEMP_GK[[2]]
G_k3 <- TEMP_GK[[3]]
G_k4 <- TEMP_GK[[4]]
G_k5 <- TEMP_GK[[5]]

nrow(G_k1)
ncol(G_k1)

#X = G;
#X_ko = G_k1;
#X_all = cbind(X,X_ko);
#X_all <- scale(X_all, scale = TRUE);
#y <- scale(y, scale = TRUE);

write.table(y_new,Path_Y, sep=',', col.names=F,   row.names=F)
write.table(Beta,Path_B, sep=',', col.names=F,   row.names=F)
write.table(G,Path_orig, sep=',', col.names=F,   row.names=F)
write.table(G_k1,Path_ko1, sep=',', col.names=F,   row.names=F)
write.table(G_k2,Path_ko2, sep=',', col.names=F,   row.names=F)
write.table(G_k3,Path_ko3, sep=',', col.names=F,   row.names=F)
write.table(G_k4,Path_ko4, sep=',', col.names=F,   row.names=F)
write.table(G_k5,Path_ko5, sep=',', col.names=F,   row.names=F)

print("End of data generation")


########################################################

#######################  LASSO  ########################

########################################################

Temp = which( Beta!= 0, arr.ind=TRUE)
NZ_True = Temp
#print(NZ_True)
p = NROW(Beta)
X_ko = G_k1;
X = G;
y <- y_new;

## 90% of the sample size
smp_size <- 0.9 * nrow(X)
## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(X)), size = smp_size)


#####################################################################################

##############         Apply Lasso on Joint data X and X_ko        ##################

#####################################################################################

X_all = cbind(X,X_ko);
X_all <- scale(X_all, scale = TRUE);
y <- scale(y, scale = TRUE);

#x_trainJ <- X_all[train_ind, ]
#x_testJ <- X_all[-train_ind, ]
#y_train <- y[train_ind]
#y_test <- y[-train_ind]

lasso_modJ = glmnet(X_all, y,alpha = 1) # Fit lasso model on training data
#plot(lasso_modJ)    # Draw plot of coefficients

cv.outJ = cv.glmnet(X_all, y, alpha = 1, nfolds = 5) # Fit lasso model on training data
#plot(cv.outJ) # Draw plot of training MSE as a function of lambda
bestlamJ = cv.outJ$lambda.min # Select lamda that minimizes training MSE

ind_best <- which(cv.outJ$lambda == bestlamJ)
Min_MSE_Val <- cv.outJ$cvm[ind_best]


paste("Best Lambda Lasso regression is:", bestlamJ) 
paste("MSE validation Lasso regression is", Min_MSE_Val) 

lasso_pred_Tr = predict(lasso_modJ, s = bestlamJ, newx = X_all) # Use best lambda to predict test data
paste("MSE train Lasso is", mean((lasso_pred_Tr - y)^2)) # Calculate train MSE 

lasso_predJ = predict(lasso_modJ, s = bestlamJ, newx = X_all) # Use best lambda to predict test data
paste("MSE ALL Lasso is", mean((lasso_predJ - y)^2)) # Calculate test MSE



######################
new_lineJ = t(c(mean((lasso_pred_Tr - y)^2),mean((lasso_predJ - y)^2), indx, Min_MSE_Val, bestlamJ))

DestinationS <- prt_dest + '/'+ ML_Type + '/'+ Gene_Type + '/' + 'Metrics_Lasso_' + Feature_Size + '_H2_'+ h2 + '_C_' + CC + '_PCausal_'+ P.causal + '.csv'
if (file.exists(DestinationS) == FALSE) 
  file.create(file=DestinationS)

write.table(new_lineJ,file=DestinationS, append=TRUE, sep=',', eol="\n", col.names=F, row.names=F)

######################


outJ = glmnet(X_all, y, alpha = 1) # Fit lasso model on full dataset
lasso_coef_J <- predict(outJ, type = "coefficients", s = bestlamJ)[1:(2*p+1),] # Display coefficients using lambda chosen by CV
W_J = abs(lasso_coef_J[2:(p+1)]) - abs(lasso_coef_J[(p+2):(2*p+1)])
# Threshold 
Target_FDR <-seq(0.01, 0.2, by=0.01) 
count <- 0
FDR_L <- numeric(length = length(Target_FDR))
Power_L <- numeric(length = length(Target_FDR))

for (val in Target_FDR) {
  count = count+1
  T_J = knockoff.threshold(W_J, fdr=val)
  Inds_J = W_J > T_J
  
  Temp_J = which( Inds_J!= FALSE, arr.ind=TRUE)
  NZ_Sel_J = Temp_J
  Total_sel_J = NROW(NZ_Sel_J)
  
  FDR_L[count] = sum(Beta[NZ_Sel_J] == 0)/max(1,Total_sel_J)
  Power_L[count] = sum(Beta[NZ_Sel_J] != 0)/sum(Beta != 0)
}

print("Joint Feature Lasso")
print(FDR_L)
print(Power_L)

new_lineL = t(c(FDR_L,Power_L,indx))

DestinationL <- prt_dest + '/'+ ML_Type + '/'+ Gene_Type + '/' + 'Stat_LassoJ_' + Feature_Size + '_H2_'+ h2 + '_C_' + CC + '_PCausal_'+ P.causal + '.csv'
if (file.exists(DestinationL) == FALSE) 
  file.create(file=DestinationL)
write.table(new_lineL,file=DestinationL, append=TRUE, sep=',', eol="\n", col.names=F, row.names=F)


########################################################

##############         Ridge        ##################

########################################################
X_all = cbind(X,X_ko);
X_all <- scale(X_all, scale = TRUE);
y <- scale(y, scale = TRUE);
#lambdas_to_try = seq(0.01, 100, length = 1000)


Ridge_modJ = glmnet(X_all, y,alpha = 0) # Fit lasso model on training data , lambda = lambdas_to_try
#plot(Ridge_modJ)    # Draw plot of coefficients

cv.outJ = cv.glmnet(X_all, y, alpha = 0, nfolds = 5) # Fit lasso model on training data , lambda = lambdas_to_try
#plot(cv.outJ) # Draw plot of training MSE as a function of lambda

bestlamJ = cv.outJ$lambda.min # Select lamda that minimizes training MSE

ind_best <- which(cv.outJ$lambda == bestlamJ)
Min_MSE_Val <- cv.outJ$cvm[ind_best]


paste("The optimal Lambda for Ridge model is: ", bestlamJ) # Calculate train MSE 
paste("The minimum validation MSE for Ridge model is: ", Min_MSE_Val) # Calculate train MSE 



Ridge_pred_Tr = predict(Ridge_modJ, s = bestlamJ, newx = X_all) # Use best lambda to predict test data
paste("MSE train Ridge is", mean((Ridge_pred_Tr - y)^2)) # Calculate train MSE 


Ridge_predJ = predict(Ridge_modJ, s = bestlamJ, newx = X_all) # Use best lambda to predict test data
paste("MSE test Ridge is", mean((Ridge_predJ - y)^2)) # Calculate test MSE



######################
new_lineJ = t(c(mean((Ridge_pred_Tr - y)^2),mean((Ridge_predJ - y)^2),indx, Min_MSE_Val, bestlamJ))

DestinationS <- prt_dest + '/'+ ML_Type + '/'+ Gene_Type + '/' + 'Metrics_Ridge_' + Feature_Size + '_H2_'+ h2 + '_C_' + CC + '_PCausal_'+ P.causal + '.csv'
if (file.exists(DestinationS) == FALSE) 
  file.create(file=DestinationS)

write.table(new_lineJ,file=DestinationS, append=TRUE, sep=',', eol="\n", col.names=F, row.names=F)

######################


outJ = glmnet(X_all, y, alpha = 0) 
Ridge_coef_J <- predict(outJ, type = "coefficients", s = bestlamJ)[1:(2*p+1),] 
W_J = abs(Ridge_coef_J[2:(p+1)]) - abs(Ridge_coef_J[(p+2):(2*p+1)])
# Threshold 
Target_FDR <-seq(0.01, 0.2, by=0.01) 
count <- 0
FDR_R <- numeric(length = length(Target_FDR))
Power_R <- numeric(length = length(Target_FDR))

for (val in Target_FDR) {
  count = count+1
  T_J = knockoff.threshold(W_J, fdr=val)
  Inds_J = W_J > T_J
  
  Temp_J = which( Inds_J!= FALSE, arr.ind=TRUE)
  NZ_Sel_J = Temp_J
  Total_sel_J = NROW(NZ_Sel_J)
  
  FDR_R[count] = sum(Beta[NZ_Sel_J] == 0)/max(1,Total_sel_J)
  Power_R[count] = sum(Beta[NZ_Sel_J] != 0)/sum(Beta != 0)
}

print("Joint Feature Ridge")
print(FDR_R)
print(Power_R)

new_lineR = t(c(FDR_R,Power_R,indx))

DestinationR <- prt_dest + '/'+ ML_Type + '/'+ Gene_Type + '/' + 'Stat_Ridge_' + Feature_Size + '_H2_'+ h2 + '_C_' + CC + '_PCausal_'+ P.causal + '.csv'
if (file.exists(DestinationR) == FALSE) 
  file.create(file=DestinationR)
  
write.table(new_lineR,file=DestinationR, append=TRUE, sep=',', eol="\n", col.names=F, row.names=F)


########################################################

##############         SVR        ##################

########################################################

library(LiblineaR)

# Center and scale data
#s=scale(x_trainJ,center=TRUE,scale=TRUE)

# Estimate MSE in cross-vaidation on a train set
MSECross=LiblineaR(data = X_all, target = y, bias = 1, type = 11, cross = 10, svr_eps=.05)

# Build the model
m=LiblineaR(data = X_all, target = y, bias = 1, type = 11, cross=0, svr_eps=.05)

#s2=scale(x_testJ,attr(s,"scaled:center"),attr(s,"scaled:scale"))
pred=predict(m,X_all)$predictions
MSETest=mean((y-pred)^2)

# Was MSE well estimated?
print(MSETest-MSECross)

# Distribution of errors
print(summary(y-pred))

coef_J <- m$W
W_J = abs(coef_J[1:(p)]) - abs(coef_J[(p+1):(2*p)])

#T_J = knockoff.threshold(W_J, fdr=0.1)
#Inds_J = W_J > T_J
#Temp_J = which( Inds_J!= FALSE, arr.ind=TRUE)
#NZ_Sel_J = Temp_J
#Total_sel_J = NROW(NZ_Sel_J)
#FDR_J = sum(Beta[NZ_Sel_J] == 0)/max(1,Total_sel_J)
#Power_J = sum(Beta[NZ_Sel_J] != 0)/sum(Beta != 0)

Target_FDR <-seq(0.01, 0.2, by=0.01) 
count <- 0
FDR_S <- numeric(length = length(Target_FDR))
Power_S <- numeric(length = length(Target_FDR))

for (val in Target_FDR) {
  count = count+1
  T_J = knockoff.threshold(W_J, fdr=val)
  Inds_J = W_J > T_J
  
  Temp_J = which( Inds_J!= FALSE, arr.ind=TRUE)
  NZ_Sel_J = Temp_J
  Total_sel_J = NROW(NZ_Sel_J)
  
  FDR_S[count] = sum(Beta[NZ_Sel_J] == 0)/max(1,Total_sel_J)
  Power_S[count] = sum(Beta[NZ_Sel_J] != 0)/sum(Beta != 0)
}
#if(val %% 2 == 0)  count = count+1
#print(count)
#plot(Target_FDR, FDR_J, pch=16, ylim=c(0,1), type="b") # first plot
print("Joint Feature")
print(FDR_S)
print(Power_S)

new_lineS = t(c(FDR_S,Power_S,indx))

DestinationS <- prt_dest + '/'+ ML_Type + '/'+ Gene_Type + '/' + 'Stat_SVR_' + Feature_Size + '_H2_'+ h2 + '_C_' + CC + '_PCausal_'+ P.causal + '.csv'
if (file.exists(DestinationS) == FALSE) 
  file.create(file=DestinationS)
  
write.table(new_lineS,file=DestinationS, append=TRUE, sep=',', eol="\n", col.names=F, row.names=F)

